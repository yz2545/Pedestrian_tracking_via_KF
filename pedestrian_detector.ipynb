{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d3551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from helpers import imu\n",
    "from matplotlib import cm\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms as transforms\n",
    "import scipy\n",
    "from scipy.io import savemat\n",
    "\n",
    "camL, camR = \"cam0\", \"cam2\"\n",
    "\n",
    "# the parameters in the root are re-optimized in the calibration between cam and lidar\n",
    "root = \"./params/\"\n",
    "RotL = np.load(os.path.join(root, camL, \"Rot.npy\"))\n",
    "TL = np.load(os.path.join(root, camL, \"T.npy\"))\n",
    "RotR = np.load(os.path.join(root, camR, \"Rot.npy\"))\n",
    "TR = np.load(os.path.join(root, camR , \"T.npy\"))\n",
    "\n",
    "extrinsicsL = np.vstack((np.hstack((RotL, TL)), np.array([[0,0,0,1]])))\n",
    "extrinsicsR = np.vstack((np.hstack((RotR, TR)), np.array([[0,0,0,1]])))\n",
    "extrinsic = np.dot(extrinsicsR, np.linalg.inv(extrinsicsL))\n",
    "R_rel = extrinsic[:3, :3]\n",
    "T_rel = extrinsic[:3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ab267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code from https://debuggercafe.com/instance-segmentation-with-pytorch-and-mask-r-cnn/\n",
    "# Load Mask RCNN Model\n",
    "coco_names = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "# this will help us create a different color for each class\n",
    "COLORS = np.random.uniform(0, 255, size=(len(coco_names), 3))\n",
    "def get_outputs(image, model, threshold):\n",
    "    with torch.no_grad():\n",
    "        # forward pass of the image through the modle\n",
    "        outputs = model(image)\n",
    "    \n",
    "    # get all the scores\n",
    "    scores = list(outputs[0]['scores'].detach().cpu().numpy())\n",
    "    # index of those scores which are above a certain threshold\n",
    "    thresholded_preds_inidices = [scores.index(i) for i in scores if i > threshold]\n",
    "    thresholded_preds_count = len(thresholded_preds_inidices)\n",
    "    # get the masks\n",
    "    masks = (outputs[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
    "    # discard masks for objects which are below threshold\n",
    "    masks = masks[:thresholded_preds_count]\n",
    "    # get the bounding boxes, in (x1, y1), (x2, y2) format\n",
    "    boxes = [[(int(i[0]), int(i[1])), (int(i[2]), int(i[3]))]  for i in outputs[0]['boxes'].detach().cpu()]\n",
    "    # discard bounding boxes below threshold value\n",
    "    boxes = boxes[:thresholded_preds_count]\n",
    "    # get the classes labels\n",
    "    labels = [coco_names[i] for i in outputs[0]['labels']]\n",
    "    # only return \"person\" detection -- Yan Zhang\n",
    "    person_index=[i for i, x in enumerate(labels[0:len(masks)]) if x=='person']\n",
    "    labels_person=[labels[i] for i in person_index]\n",
    "    masks_person=[masks[i] for i in person_index]\n",
    "    boxes_person=[boxes[i] for i in person_index]\n",
    "    \n",
    "    return masks_person, boxes_person, labels_person\n",
    "    \n",
    "def draw_segmentation_map(image, masks, boxes, labels):\n",
    "    alpha = 1 \n",
    "    beta = 0.6 # transparency for the segmentation map\n",
    "    gamma = 0 # scalar added to each sum\n",
    "    for i in range(len(masks)):\n",
    "        red_map = np.zeros_like(masks[i]).astype(np.uint8)\n",
    "        green_map = np.zeros_like(masks[i]).astype(np.uint8)\n",
    "        blue_map = np.zeros_like(masks[i]).astype(np.uint8)\n",
    "        # apply a randon color mask to each object\n",
    "        color = COLORS[random.randrange(0, len(COLORS))]\n",
    "        red_map[masks[i] == 1], green_map[masks[i] == 1], blue_map[masks[i] == 1]  = color\n",
    "        # combine all the masks into a single image\n",
    "        segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n",
    "        # #convert the original PIL image into NumPy format\n",
    "        # image = np.array(image)\n",
    "        # # convert from RGN to OpenCV BGR format\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # apply mask on the image\n",
    "        cv2.addWeighted(image, alpha, segmentation_map, beta, gamma, image)\n",
    "        # draw the bounding boxes around the objects\n",
    "        cv2.rectangle(image, boxes[i][0], boxes[i][1], color=color, \n",
    "                      thickness=2)\n",
    "        # put the label text above the objects\n",
    "        cv2.putText(image , labels[i], (boxes[i][0][0], boxes[i][0][1]-10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, color, \n",
    "                    thickness=2, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def get_detections(image, model, device, threshold=0.965, visualize=True):\n",
    "    # image = Image.open(image_path).convert('RGB')\n",
    "    # keep a copy of the original image for OpenCV functions and applying masks\n",
    "    input_image = image.copy()\n",
    "    orig_image = image.copy()\n",
    "    orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "    # transform the image\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "    input_image = transform(input_image)\n",
    "    # add a batch dimension\n",
    "    input_image = input_image.unsqueeze(0).to(device)\n",
    "    \n",
    "    masks, boxes, labels = get_outputs(input_image, model, threshold)\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(25, 10))\n",
    "        result = draw_segmentation_map(orig_image, masks, boxes, labels)\n",
    "        plt.imshow(result)\n",
    "        plt.show()\n",
    "    return masks, boxes, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6fa043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, progress=True, \n",
    "                                                           num_classes=91)\n",
    "# set the computation device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load the model on to the computation device and set to eval mode\n",
    "model.to(device).eval()\n",
    "# transform to convert the image to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c085eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_split(img, width=3):\n",
    "    img = img.copy()\n",
    "    h, w, _ = img.shape\n",
    "    interval = int(h / 20)\n",
    "    for i in range(-1, h, interval*2):\n",
    "        img[i-width:i+width, :, :] = np.array([255])\n",
    "    for i in range(interval, h, interval*2):\n",
    "        img[i-width:i+width, :, :] = np.array([128])\n",
    "    return img\n",
    "\n",
    "def transform_points(pts_3d_ref, Tr):\n",
    "    pts_3d_ref = cart2hom(pts_3d_ref)  # nx4\n",
    "    return np.dot(pts_3d_ref, np.transpose(Tr)).reshape(-1, 4)[:, 0:3]\n",
    "    \n",
    "def load_velo_scan(velo_filename, lidar_channels=4, dtype=np.float32):\n",
    "    scan = np.fromfile(velo_filename, dtype=dtype)\n",
    "    scan = scan.reshape((-1, lidar_channels))\n",
    "    return scan\n",
    "\n",
    "def com(R, ex):\n",
    "    return np.dot(np.linalg.inv(R), ex)\n",
    "\n",
    "\n",
    "def rectify_image(left_img, right_img, R, T, instrinsicL, instrinsicR, distCoeffL, distCoeffR):\n",
    "    assert left_img.shape == right_img.shape\n",
    "    STEREO_IMG_HEIGHT, STEREO_IMG_WIDTH, _ = left_img.shape\n",
    "    \n",
    "#     distCoeffL = distCoeffR = np.zeros(4)\n",
    "\n",
    "    R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(\n",
    "        cameraMatrix1=instrinsicL,\n",
    "        distCoeffs1=distCoeffL,\n",
    "        cameraMatrix2=instrinsicR,\n",
    "        distCoeffs2=distCoeffR,\n",
    "        imageSize=(STEREO_IMG_WIDTH, STEREO_IMG_HEIGHT),\n",
    "        R=R,\n",
    "        T=T,\n",
    "        flags=0,\n",
    "        alpha=0\n",
    "    )\n",
    "\n",
    "    map1x, map1y = cv2.initUndistortRectifyMap(\n",
    "        cameraMatrix=instrinsicL,\n",
    "        distCoeffs=distCoeffL,\n",
    "        R=R1,\n",
    "        newCameraMatrix=P1,\n",
    "        size=(STEREO_IMG_WIDTH, STEREO_IMG_HEIGHT),\n",
    "        m1type=cv2.CV_32FC1)\n",
    "\n",
    "    map2x, map2y = cv2.initUndistortRectifyMap(\n",
    "        cameraMatrix=instrinsicR,\n",
    "        distCoeffs=distCoeffR,\n",
    "        R=R2,\n",
    "        newCameraMatrix=P2,\n",
    "        size=(STEREO_IMG_WIDTH, STEREO_IMG_HEIGHT),\n",
    "        m1type=cv2.CV_32FC1)\n",
    "\n",
    "    extrinsic = np.zeros((3, 4))\n",
    "    extrinsic[:3, :3] = R\n",
    "    extrinsic[:3, 3] = T\n",
    "    extrinsic = np.dot(R1, extrinsic)  # calibrate with R1\n",
    "\n",
    "    left_img_rect = cv2.remap(left_img, map1x, map1y, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)\n",
    "    right_img_rect = cv2.remap(right_img, map2x, map2y, cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)\n",
    "\n",
    "    return P1, P2, left_img_rect, right_img_rect, R1, R2, extrinsic, Q\n",
    "\n",
    "def project_rect_to_image(pts_3d_rect,P):\n",
    "        ''' Input: nx3 points in rect camera coord.\n",
    "            Output: nx2 points in image2 coord.\n",
    "        '''\n",
    "        pts_3d_rect = cart2hom(pts_3d_rect)\n",
    "        pts_2d = np.dot(pts_3d_rect, np.transpose(P)) # nx3\n",
    "        pts_2d[:,0] /= pts_2d[:,2]\n",
    "        pts_2d[:,1] /= pts_2d[:,2]\n",
    "        return pts_2d[:,0:2]\n",
    "    \n",
    "def cart2hom(pts_3d):\n",
    "        ''' Input: nx3 points in Cartesian\n",
    "            Oupput: nx4 points in Homogeneous by pending 1\n",
    "        '''\n",
    "        n = pts_3d.shape[0]\n",
    "        pts_3d_hom = np.hstack((pts_3d, np.ones((n,1))))\n",
    "        return pts_3d_hom\n",
    "    \n",
    "def load_pcl_from_bin5(bin_file_path):\n",
    "    bin_pcd = np.fromfile(bin_file_path,dtype=np.float64)\n",
    "    points = bin_pcd.reshape(-1,5)\n",
    "\n",
    "    return points[:,:3],points[:,3],points[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9f5ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load camera to camera parameters\n",
    "configs = [\"02\"] # left most: 0 right most: 1\n",
    "\n",
    "root = \"./\"\n",
    "img_root =\"./\"\n",
    "intrinsic_root = \"./params\"\n",
    "\n",
    "cams = os.listdir(root)\n",
    "\n",
    "\n",
    "instrinsicL = np.load(os.path.join(intrinsic_root, camL, \"camera_matrix.npy\"))\n",
    "instrinsicR = np.load(os.path.join(intrinsic_root, camR, \"camera_matrix.npy\"))\n",
    "\n",
    "distCoeffL = np.load(os.path.join(intrinsic_root, camL, \"distortion_coefficients.npy\"))\n",
    "distCoeffR = np.load(os.path.join(intrinsic_root, camR, \"distortion_coefficients.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00806785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yan's laptop\\AppData\\Local\\Temp\\ipykernel_15064\\2452156571.py:73: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize = (120,10))\n",
      "C:\\Users\\Yan's laptop\\AppData\\Local\\Temp\\ipykernel_15064\\2452156571.py:38: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(25, 10))\n",
      "c:\\ProgramData\\Anaconda3\\envs\\stereo\\lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:493: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  narr = np.asanyarray(source)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap=cm.get_cmap('jet')\n",
    "# pick difficulty\n",
    "load_path = \"./drives/easy/\"\n",
    "# load_path = \"./drives/medium/\"\n",
    "# load_path = \"./drives/hard/\"\n",
    "\n",
    "frames = np.load(load_path + \"data.npy\", allow_pickle=True) # list of dicts containing sycronized images lidar and imu\n",
    "# [{\"left_img\": \"\", \"right_img\": \"\", \"imu\": [], \"lidar\": \"\"}, ...]\n",
    "# imu is array like with\n",
    "# the following structure\n",
    "# [latitude, longitude, height, roll, pitch, azimuth, east_velocity, north_velocity, up_velocity]\n",
    "\n",
    "stereo = cv2.StereoBM_create(numDisparities=128, blockSize=5) # stereo disparity class\n",
    "# create a list of dictionary that stores the lidar measurement of each detected person\n",
    "pedestrian_lidar_x=[]\n",
    "pedestrian_lidar_y=[]\n",
    "pedestrian_lidar_z=[]\n",
    "for i, frame in enumerate(frames):\n",
    "    # if i !=1:\n",
    "    #     continue\n",
    "    imagesL = frame[\"left_img\"]\n",
    "    imagesR = frame[\"right_img\"]\n",
    "    lidar_file = frame[\"lidar\"]\n",
    "    imu_data = frame[\"imu\"] \n",
    "    imgL = cv2.imread(imagesL)\n",
    "    imgR = cv2.imread(imagesR)\n",
    "    img_file = imagesL\n",
    "\n",
    "    pcl = load_velo_scan(lidar_file, lidar_channels=5, dtype=np.float64) #Nx5 [x,y,z,i,t] np.float.64\n",
    "    calibL, calibR, left_img_rect, right_img_rect, R1, R2, extrinsics, Q = rectify_image(imgL, imgR, R_rel, T_rel, instrinsicL, instrinsicR, distCoeffL, distCoeffR)\n",
    "#     img_combine = line_split(np.hstack([left_img_rect, right_img_rect]))\n",
    "    img_combine = np.hstack([left_img_rect, right_img_rect])\n",
    "    # get image detections for left image\n",
    "    masksL, boxesL, labelsL = get_detections(left_img_rect, model, device, threshold=0.9, visualize=False)\n",
    "    # get image detections for right image\n",
    "    # masksR, boxesR, labelsR = get_detections(right_img_rect, model, device, threshold=0.9, visualize=True)\n",
    "    \n",
    "    plt.figure(figsize=(25, 10))\n",
    "    # disparity = stereo.compute(cv2.cvtColor(left_img_rect, cv2.COLOR_BGR2GRAY), cv2.cvtColor(right_img_rect, cv2.COLOR_BGR2GRAY))\n",
    "    # plt.imshow(disparity,'gray')\n",
    "    # plt.show()\n",
    "\n",
    "    # lidar data\n",
    "    pts, intensity, times = load_pcl_from_bin5(lidar_file)\n",
    "\n",
    "    pts_cam_coord = np.dot(RotL,pts.T) + TL.reshape(-1,1) \n",
    "    # rectify\n",
    "    pts_cam_coord = np.dot(R1,pts_cam_coord )\n",
    "    pts_cam_coord  = pts_cam_coord.T\n",
    "\n",
    "    pts = pts[pts_cam_coord [:,2]>=0,:] \n",
    "    pts_cam_coord  = pts_cam_coord [pts_cam_coord [:,2]>=0,:]\n",
    "    pts = pts[pts_cam_coord [:,2]<=30,:] \n",
    "    pts_cam_coord  = pts_cam_coord [pts_cam_coord [:,2]<=30,:]\n",
    "\n",
    "    pixels = project_rect_to_image(pts_cam_coord, calibL)\n",
    "\n",
    "\n",
    "    u = pixels[:,0]\n",
    "    v = pixels[:,1]\n",
    "\n",
    "    idx = (u>0)*(u<1920-1)*(v>0)*(v<1208-1)\n",
    "    u = u[idx]\n",
    "    v=v[idx]\n",
    "    pts_captured=pts[idx]\n",
    "    # transform from lidar coordinate to imu coordinate\n",
    "    TL2I = np.array([[-0.70497776,  0.70921071, -0.00515127,  0.00902472],\n",
    "                    [-0.70354996, -0.69839657,  0.13137612,  1.09214766],\n",
    "                    [ 0.08957573,  0.09624142,  0.99131921,  1.00137747],\n",
    "                    [ 0.        ,  0.        ,  0.        ,  1.        ]])\n",
    "    pts_captured=transform_points(pts_captured, TL2I)\n",
    "\n",
    "    plt.figure(figsize = (120,10))\n",
    "    plt.imshow(cv2.cvtColor(left_img_rect, cv2.COLOR_BGR2RGB)) #cv2 is BGR image, matplotlib is RGB\n",
    "    plt.scatter(u,v,c=pts_cam_coord[idx,2], s = 2,cmap = cmap)\n",
    "\n",
    "\n",
    "    plt.close()\n",
    "    # calculate the average lidar measurement of each detected person\n",
    "    # round u and v to integers\n",
    "    u_int=np.rint(u)\n",
    "    v_int=np.rint(v)\n",
    "    lidar_pts=pts_cam_coord[idx]\n",
    "    # create an zero array\n",
    "    lidar_arr_x=np.zeros((1208,1920),dtype=float,order='C')\n",
    "    lidar_arr_y=np.zeros((1208,1920),dtype=float,order='C')\n",
    "    lidar_arr_z=np.zeros((1208,1920),dtype=float,order='C')\n",
    "    for i,x in enumerate(u):\n",
    "        # lidar_arr[int(v[i])][int(u[i])]=lidar_pts[i,2] # np.sqrt(lidar_pts[i,0]**2+lidar_pts[i,1]**2+lidar_pts[i,2]**2)\n",
    "        lidar_arr_x[int(v[i])][int(u[i])]=pts_captured[i,0]\n",
    "        lidar_arr_y[int(v[i])][int(u[i])]=pts_captured[i,1]\n",
    "        lidar_arr_z[int(v[i])][int(u[i])]=pts_captured[i,2]\n",
    "    # calculate the average measurement\n",
    "    zx_mean=np.zeros(len(labelsL))\n",
    "    zy_mean=np.zeros(len(labelsL))\n",
    "    zz_mean=np.zeros(len(labelsL))\n",
    "\n",
    "    for i,label in enumerate(labelsL):\n",
    "        zx=np.multiply(masksL[i],lidar_arr_x)\n",
    "        zy=np.multiply(masksL[i],lidar_arr_y)\n",
    "        zz=np.multiply(masksL[i],lidar_arr_z)\n",
    "        zx_mean[i]=np.mean(zx[np.nonzero(zx)])\n",
    "        zy_mean[i]=np.mean(zy[np.nonzero(zy)])\n",
    "        zz_mean[i]=np.mean(zz[np.nonzero(zz)])\n",
    "    # save average lidar measurement \n",
    "    pedestrian_lidar_x.append(zx_mean)\n",
    "    pedestrian_lidar_y.append(zy_mean)\n",
    "    pedestrian_lidar_z.append(zz_mean)\n",
    "\n",
    "  \n",
    "lidar_mdic={'x':pedestrian_lidar_x,'y':pedestrian_lidar_y,'z':pedestrian_lidar_z}  \n",
    "scipy.io.savemat('pedestrian_lidar_measurement.mat',lidar_mdic)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de20574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stereo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6378b7c6aa0079f520eef35416d546071e414fc16b724ce259c8e0223e23912"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
